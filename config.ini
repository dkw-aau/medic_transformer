[general]
; [corpus, pre_train, fine_tune]
task = pre_train

[files]
; [corpus, corpus_half, corpus_small]
corpus_name = corpus_half
pretrain_name = MLM_Model.pt
finetune_name = FT_Model.pt

[globals]
step_eval = 5

[optimization]
lr = 0.00003
warmup_proportion = 0.1
weight_decay = 0.01

[train_params]
max_epochs = 50
batch_size = 256
max_len_seq = 256
use_gpu = false

[model_params]
; 288
hidden_size = 64
layer_dropout = 0.1
num_hidden_layers = 2
num_attention_heads = 4
att_dropout = 0.1
intermediate_size = 512
hidden_act = gelu
initializer_range = 0.02

[extraction]
prepare_parquet = false
max_sequences = -1
