[general]
; [corpus, baseline, history, pre_train, fine_tune]
task = history

[neptune]
use_logging = true
neptune_project_id = iknowlogic/Medic-Transformer
neptune_token_key = neptune_api_token

experiment_name = real_full

[files]
corpus_name = corpus

[optimization]
lr = 0.00001
warmup_proportion = 0.1
weight_decay = 0.003

[train_params]
max_epochs = 1000
batch_size = 128
max_len_seq = 256
use_gpu = true
use_pretrained = false
save_model = true
patience = 10

[model_params]
hidden_size = 288
layer_dropout = 0.1
num_hidden_layers = 6
num_attention_heads = 8
att_dropout = 0.1
intermediate_size = 288
hidden_act = gelu
initializer_range = 0.02

[baseline]
use_saved = true
hours = 24
;{min,max,min-max,avg}
strategy = min-max
;{none,mean,median}
imputation = mean
;{standard,min-max}
scaler = min-max
;{chi2}
feature_select = chi2
;{rfc,nn,svc}
cls = nn

[extraction]
prepare_parquet = false
max_sequences = -1



